<!doctype HTML>
<html>
<head>
<meta charset="utf-8">
<title>web work notes</title>
<style>
h1, h2, h3, h4, h5 {font-weight: normal !important;}
h3 {background-color: yellow;}
code {color: blue;}
ul li, ol li {margin: 5px 0;}
body {padding-right: 30%; padding-bottom: 120px; padding-top: 30px;}
nav {
position: fixed;
top: 0;
width: 100%;
left: 0;
z-index: 2;
padding: 4px;
}
nav a {
color: red !important;
padding: 0 5px;
}
</style>

</head>
<body>
<nav>
<a href="index.html">www</a>
<a href="notes.html">technical notes</a>
<a href="https://docs.google.com/document/d/1lceNE5sAygto2zyg4u26MldpmAMoys2OVyvCPnChOzI/edit?usp=sharing" target="_blank">syllabus</a>
<a href="info.html">info</a> 
</nav>
<div class="frame top"></div>
<div class="frame left"></div>
<div class="frame right"></div>

<h2>on computers & memory</h2>
<p>
When I first started to think through computers and memory, I had been thinking a lot about computation and metaphor. More specifically how the metaphor of the desktop was claimed along with the personal computer as “for everyone” but really it was for everyone who was already familiar with the system of the office.  
<br>
And to this day I am perpetually perplexed that nearly everything I do as a computer user is somehow removed from what my computer is physically doing and instead I am always working within sets of metaphors, and with them sets of ideologies, to remember and understand how to do those things within this visual interface of the desktop that were all looking into right now. I first read **Wendy Chun**’s essay “On Software, or the Persistence of Visual Knowledge” in a class taught by **American Artist** and it really opened up the history of computing for me. In discussing software as ideology Chun writes “Software, or perhaps more precisely operating systems, offer us an imaginary relationship to our hardware: they do not represent transistors but rather desktops and recycling bins. Software produces ‘users.’” 
<br>
My own current research focuses on the history of the computer mouse and I have found it helpful to look at this, which is simultaneously a history of, in Chun’s words, “ideology machines”, in order to understand how in one way or another we got to the computers we are all using today. There is so much history there but for now I want to share with you a short pre-history of the mouse and its relationship to the screen. This is kind of a roundabout way to get to computing and memory but for me they are philosophically and materially intertwined. 
<br>
Early screen technology saw a long line of input devices used for pointing, tracking, choosing, and classifying. These input devices, which included the mouse, stylus pens, light guns, light pens, are rooted in the emergence of screen technology. This is long before the technology of touch, in which the hand became the input device itself.  
<br>
The origins of the input device, being an input device as we know it, belong to a cold war computer called the Whirwind. This was the first computer to make its calculations in real time (Edwards). Before the Whirlwind, computers would take really long periods of time to process a program and would then require the physical adjustment of switches or punched cards, a re-programming, in order to run a process again. Originally designed as a flight simulator, the Whirlwind was able to receive inputs and produce outputs using a three-dimensional array of magnetic core memory. Magentic core memory is a kind of hardware and software design using magnetic wires which allowed for a program to be stored even if power was lost...so if the computer was turned off, the programming of the wires would still be intact. This technology was used in tandem with vacuum tube screens which in itself allowed for a graphical interface of computed data. **Jacob Gaboury** on the history of the computer screen writes “The screen is not simply an enduring technique or evocative metaphor; it is a hardware object whose transformations have shaped the material conditions of our visual culture.” 
<br>
Because CRT (cathode ray tube) screens, standardized by the 1950s, transformed ones and zeroes into light and in real time, they required an input device which shared the language of light as signal. This is where the mouse’s predecessor, the light pen, materialized (Edwards). At the time of the light pen, computers were already much more than calculating machines, but they were not yet ubiquitously used for purposes outside of the military. The screen was still thought of as a substrate like paper, the analog substrate for tracking and storing, and the light pen was the next best iteration of moving from the analog act of writing to the digital act of pointing. In this sense, the problem of computing became not only the act of programming switches but also the act of moving on along x and y coordinates. 
<br>
Ok so fast forward to the 1960’s, screens are becoming standardized and research towards the project of what computers *could* and *should* do had forked (Bardini). In one direction the idea was to build up an autonomous intelligence that would match or even supersede human intelligence. The question, by the way, of “what intelligence actually is” still stands and in step with philosopher Hubert Dreyfus I think that artificial intelligence has proven to show us what computer can't do than what they can. In the other direction, a person named Douglas Engelbart led a research lab called the Augementation Research Center which sought to create a kind of intelligent symbiosis ~between~ the human and the computer. 
<br>
It is in between the human and the computer where the mouse, made for the human hand, first materialized. Invented by Engelbart in 1964 the mouse was but a small part of a much larger system. Presented to the public as what later became known as “the mother of all demoes” the system that Engelbart and his colleagues built included some of the earliest versions of hyperlinks, multiple windows, real-time cross-computer collaboration (which is what we will be doing today), and finally, the use of a mouse. It was an amalgamation of devices, all situated within optimal distance for the hand to the mouse, the eyes to the screen. 
<br>
Later in the 1970's the mouse was re-designed so that it could be used alongside the Xerox Alto, an early personal computer  marketed as an office system. Ok so here we are, back to the screen, back to the desktop. I like to think that the mouse brought us here. All of your office needs could be met by a series of keystrokes and clicks. And at this point, the mouse had become the standard input device for computing and was synonymous not only with interactivity and collaboration but also with work. 
<br>
This desktop metaphor ensured that computers require programming, as we know it today, to be a set of extremely abstract processes. So, when I’m coding something, I’m often coding on top of someone else’s code, in someone else’s software, on top of the software that makes up my operating system which is permanently baked into the proprietary silicon chip of my Apple computer. Computers at their core still consist of a complex series of on and off switches, ones and zeroes except now the switches are like an extremely tiny mix of analog and digital signals and are nearly invisible to the human eye. This means that everything we do on our computers today is already abstracting away those complex patterns of numbers in order for us to use them in the way that we do. So these abstractions allow us to do things like writing text in a file, without thinking about how or even if the computer will be able to do this. 
<br>
I think it is important to note that since the early days of computing computer time has always been more important than human time. When programmers were writing code for computers the size of rooms the time it took to run the program was really expensive, so people were always writing code that was most efficiently read by the computer itself not by the people who were making the programs. This reflects our current moment of computing where code is written in often illegible jargon and minified into unreadable blobs of text. This is why coding is intimidating, it seems to be a problem of language, some kind of insider’s club, but really it’s just been purposefully abstracted and a lot of the original languages from the 70s, 80s and 90s are still the foundation upon which so much code is written. As an example, today when someone builds a website the might use a mix of languages like Ruby, JavaScript and Python and then uses a mix of libraries and frameworks which someone else has written to compile all of it’s into the only three readable languages by the browser HTML, CSS, and JavaScript all of which haven’t changed in any major way since the mid 90s. 
<br>
Going back to this idea of the desktop metaphor. The idea of a file inside a folder on a desktop...and then there's the trashcan. I’m always wondering, when you put something in the trash, does it ever really go away? And what is the file really? And what is the folder? a computer scientist friend once told me that folders aren't actually anything. They don't take up *any* space on your computer. I often reflect on something Kameelah Janan Rasheed once said: "analogies are a poverty of language." In step with this provocation I am always wondering how the metaphors of files and folders, the analogies we use in our every day computing, have limited our imagination for what computing can be? If not for the baked in interface of the desktop what might we put on a screen emitting light? Or to think about it in a different way, what would it be like if we could hear all of the computation happening inside our laptops or our phones as we typed out an email? How might we approach the act of computing differently? 
<br>
This is all just to interrogate the fact that there has been a significant shift from physical programming, the act of physically adjusting switches or weaving software into core rope memory to abstracted programming, building software on top of software. And with that abstraction 
</p>


</body>
</html>